{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPS2TJw18CMxp6MLPhqmLIc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BF667/ipynb/blob/main/LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Fine-tuning with TRL\n",
        "\n",
        "This notebook demonstrates fine-tuning language models using TRL (Transformer Reinforcement Learning) library."
      ],
      "metadata": {
        "id": "Xq8W5Q6dWz4c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jx7WHY6RVEM8"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch transformers datasets tokenizers trl peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Disable Weights & Biases\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "d8vGJ-kUW2ND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "MODEL_NAME = \"microsoft/DialoGPT-small\"  # You can change this to any causal LM\n",
        "DATASET_NAME = \"timdettmers/openassistant-guanaco\"  # Example dataset for instruction tuning\n",
        "OUTPUT_DIR = \"./results\"\n",
        "MAX_SEQ_LENGTH = 512\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set pad token"
      ],
      "metadata": {
        "id": "k9V2c4a_W5dO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and prepare dataset\n",
        "dataset = load_dataset(DATASET_NAME)\n",
        "\n",
        "# Format the dataset for instruction tuning\n",
        "def format_instruction(example):\n",
        "    return {\"text\": f\"### Instruction: {example['instruction']}\\n### Response: {example['response']}\"}\n",
        "\n",
        "dataset = dataset.map(format_instruction)\n",
        "\n",
        "print(\"Dataset sample:\")\n",
        "print(dataset['train'][0]['text'])"
      ],
      "metadata": {
        "id": "q9b8pQy_W8vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantization configuration for memory efficiency (optional)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Load model with quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")"
      ],
      "metadata": {
        "id": "vP4UQ8HXXA7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA configuration for efficient fine-tuning\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"v_proj\"]  # Adjust based on your model architecture\n",
        ")"
      ],
      "metadata": {
        "id": "4Dm8s4L6XDVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=10,\n",
        "    save_steps=500,\n",
        "    fp16=True,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    report_to=\"none\",  # Disable all logging integrations including wandb\n",
        "    remove_unused_columns=False,\n",
        ")"
      ],
      "metadata": {
        "id": "8uV5U0GzXGDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    tokenizer=tokenizer,\n",
        "    peft_config=peft_config,\n",
        "    packing=True,  # Efficiently pack multiple sequences\n",
        ")"
      ],
      "metadata": {
        "id": "7mQK4z_OXI8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "vjvG3bXIXL7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "trainer.save_model()\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(\"Model saved successfully!\")"
      ],
      "metadata": {
        "id": "o6V6yV3xXOT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the trained model\n",
        "def generate_response(prompt, max_length=100):\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Test with a sample prompt\n",
        "test_prompt = \"### Instruction: Explain what machine learning is.\\n### Response:\"\n",
        "response = generate_response(test_prompt)\n",
        "print(\"Generated response:\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "hQw6vKbZXQpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternative: Using your own text file dataset\n",
        "# Uncomment and modify the following code if you want to use your own text file:\n",
        "\n",
        "# from datasets import load_dataset\n",
        "# \n",
        "# # Load your custom text file\n",
        "# custom_dataset = load_dataset(\"text\", data_files={\"train\": \"/content/your_data.txt\"})\n",
        "# \n",
        "# # For custom text files, you might want to use a different formatting function\n",
        "# def format_custom_text(example):\n",
        "#     return {\"text\": example[\"text\"]}\n",
        "# \n",
        "# custom_dataset = custom_dataset.map(format_custom_text)\n",
        "# \n",
        "# # Then use custom_dataset in the SFTTrainer instead of dataset[\"train\"]"
      ],
      "metadata": {
        "id": "qXv2mWJAXTQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Features of This Notebook:\n",
        "\n",
        "1. **TRL Integration**: Uses `SFTTrainer` from TRL for supervised fine-tuning\n",
        "2. **No WandB**: Completely disabled Weights & Biases logging\n",
        "3. **QLoRA Support**: Uses 4-bit quantization for memory efficiency\n",
        "4. **LoRA Fine-tuning**: Efficient parameter-efficient fine-tuning\n",
        "5. **Instruction Tuning**: Formatted for instruction-response datasets\n",
        "6. **Memory Optimized**: Uses gradient accumulation and mixed precision\n",
        "\n",
        "## To Use Your Own Data:\n",
        "\n",
        "1. Upload your text file to Colab\n",
        "2. Uncomment and modify the last cell\n",
        "3. Replace `DATASET_NAME` with your file path\n",
        "4. Adjust the formatting function as needed"
      ],
      "metadata": {
        "id": "k8uZ1p8MXV7V"
      }
    }
  ]
}
