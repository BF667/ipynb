{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Fine-tuning with TRL - Complete Toolkit\n",
        "\n",
        "Complete toolkit for fine-tuning language models with dataset creation, safeguard management, and advanced features."
      ],
      "metadata": {
        "id": "title-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install torch transformers datasets tokenizers trl peft accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "install-packages"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "import pandas as pd\n",
        "import gc\n",
        "from datasets import Dataset, load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    BitsAndBytesConfig,\n",
        "    GenerationConfig\n",
        ")\n",
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig, TaskType\n",
        "\n",
        "# Disable Weights & Biases\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "print(\"All packages imported successfully!\")"
      ],
      "metadata": {
        "id": "import-packages"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Creation Tools"
      ],
      "metadata": {
        "id": "dataset-tools"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DatasetCreator:\n",
        "    \"\"\"Tool for creating training datasets from various sources\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def from_text_file(file_path, chunk_size=512, overlap=50):\n",
        "        \"\"\"Create dataset from a plain text file\"\"\"\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "        \n",
        "        chunks = []\n",
        "        start = 0\n",
        "        text_length = len(text)\n",
        "        \n",
        "        while start < text_length:\n",
        "            end = start + chunk_size\n",
        "            chunk = text[start:end]\n",
        "            chunks.append({\"text\": chunk})\n",
        "            start += chunk_size - overlap\n",
        "        \n",
        "        return Dataset.from_list(chunks)\n",
        "    \n",
        "    @staticmethod\n",
        "    def from_instruction_pairs(instructions, responses, system_prompt=None):\n",
        "        \"\"\"Create dataset from instruction-response pairs\"\"\"\n",
        "        data = []\n",
        "        for instr, resp in zip(instructions, responses):\n",
        "            if system_prompt:\n",
        "                formatted_text = f\"### System: {system_prompt}\\n### Instruction: {instr}\\n### Response: {resp}\"\n",
        "            else:\n",
        "                formatted_text = f\"### Instruction: {instr}\\n### Response: {resp}\"\n",
        "            data.append({\"text\": formatted_text})\n",
        "        \n",
        "        return Dataset.from_list(data)\n",
        "    \n",
        "    @staticmethod\n",
        "    def from_jsonl(file_path, text_field=\"text\"):\n",
        "        \"\"\"Create dataset from JSONL file\"\"\"\n",
        "        data = []\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                item = json.loads(line)\n",
        "                data.append({\"text\": item[text_field]})\n",
        "        \n",
        "        return Dataset.from_list(data)\n",
        "    \n",
        "    @staticmethod\n",
        "    def from_csv(file_path, text_column=\"text\"):\n",
        "        \"\"\"Create dataset from CSV file\"\"\"\n",
        "        df = pd.read_csv(file_path)\n",
        "        data = [{\"text\": str(row[text_column])} for _, row in df.iterrows()]\n",
        "        return Dataset.from_list(data)\n",
        "    \n",
        "    @staticmethod\n",
        "    def create_conversation_dataset(conversations, system_message=None):\n",
        "        \"\"\"Create dataset from conversation history\"\"\"\n",
        "        data = []\n",
        "        for conv in conversations:\n",
        "            formatted_conv = \"\"\n",
        "            if system_message:\n",
        "                formatted_conv += f\"System: {system_message}\\n\"\n",
        "            formatted_conv += \"\\n\".join([f\"{role}: {text}\" for role, text in conv])\n",
        "            data.append({\"text\": formatted_conv})\n",
        "        \n",
        "        return Dataset.from_list(data)\n",
        "\n",
        "dataset_creator = DatasetCreator()\n",
        "print(\"Dataset Creator initialized!\")"
      ],
      "metadata": {
        "id": "dataset-creator"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Safeguard Management & Advanced Tools"
      ],
      "metadata": {
        "id": "safeguard-tools"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelSafeguardManager:\n",
        "    \"\"\"Tools for managing model safeguards and safety features\"\"\"\n",
        "    \n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.original_generation_config = None\n",
        "        \n",
        "    def remove_safety_filters(self):\n",
        "        \"\"\"Remove safety filters from the model (use with caution)\"\"\"\n",
        "        print(\"Removing safety filters...\")\n",
        "        \n",
        "        self.original_generation_config = self.model.generation_config\n",
        "        \n",
        "        permissive_config = GenerationConfig.from_dict({\n",
        "            \"do_sample\": True,\n",
        "            \"temperature\": 0.8,\n",
        "            \"top_p\": 0.95,\n",
        "            \"top_k\": 50,\n",
        "            \"max_new_tokens\": 512,\n",
        "            \"repetition_penalty\": 1.1,\n",
        "            \"pad_token_id\": self.tokenizer.eos_token_id,\n",
        "            \"eos_token_id\": self.tokenizer.eos_token_id,\n",
        "        })\n",
        "        \n",
        "        self.model.generation_config = permissive_config\n",
        "        print(\"Safety filters removed - model is now in permissive mode\")\n",
        "        \n",
        "    def restore_safety_filters(self):\n",
        "        \"\"\"Restore original safety filters\"\"\"\n",
        "        if self.original_generation_config:\n",
        "            self.model.generation_config = self.original_generation_config\n",
        "            print(\"Safety filters restored\")\n",
        "        else:\n",
        "            print(\"No original config found to restore\")\n",
        "    \n",
        "    def set_permissive_generation_config(self, **kwargs):\n",
        "        \"\"\"Set custom permissive generation parameters\"\"\"\n",
        "        default_config = {\n",
        "            \"do_sample\": True,\n",
        "            \"temperature\": 0.9,\n",
        "            \"top_p\": 0.95,\n",
        "            \"top_k\": 0,\n",
        "            \"max_new_tokens\": 1024,\n",
        "            \"repetition_penalty\": 1.0,\n",
        "            \"no_repeat_ngram_size\": 0,\n",
        "        }\n",
        "        default_config.update(kwargs)\n",
        "        \n",
        "        permissive_config = GenerationConfig.from_dict(default_config)\n",
        "        self.model.generation_config = permissive_config\n",
        "        print(\"Custom permissive generation config applied\")\n",
        "\n",
        "class ModelEnhancer:\n",
        "    \"\"\"Tools for enhancing model capabilities and features\"\"\"\n",
        "    \n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def optimize_for_inference(self):\n",
        "        \"\"\"Optimize model for faster inference\"\"\"\n",
        "        print(\"Optimizing model for inference...\")\n",
        "        self.model.eval()\n",
        "        if hasattr(self.model, 'config'):\n",
        "            self.model.config.use_cache = True\n",
        "        print(\"Model optimized for inference\")\n",
        "\n",
        "class AdvancedInference:\n",
        "    \"\"\"Advanced inference tools with multiple generation strategies\"\"\"\n",
        "    \n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def generate_creative(self, prompt, max_length=200, temperature=0.9):\n",
        "        \"\"\"Generate creative text with high temperature\"\"\"\n",
        "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                inputs,\n",
        "                max_length=max_length,\n",
        "                temperature=temperature,\n",
        "                do_sample=True,\n",
        "                top_p=0.95,\n",
        "                top_k=50,\n",
        "                repetition_penalty=1.1,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    def generate_deterministic(self, prompt, max_length=200):\n",
        "        \"\"\"Generate deterministic text with greedy decoding\"\"\"\n",
        "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                inputs,\n",
        "                max_length=max_length,\n",
        "                do_sample=False,\n",
        "                num_beams=1,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    def batch_generate(self, prompts, max_length=100):\n",
        "        \"\"\"Generate responses for multiple prompts\"\"\"\n",
        "        inputs = self.tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(self.model.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_length=max_length,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        responses = []\n",
        "        for i, output in enumerate(outputs):\n",
        "            response = self.tokenizer.decode(output, skip_special_tokens=True)\n",
        "            response = response[len(prompts[i]):].strip()\n",
        "            responses.append(response)\n",
        "        \n",
        "        return responses\n",
        "\n",
        "print(\"Advanced tools classes defined!\")"
      ],
      "metadata": {
        "id": "advanced-tools"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Sample Dataset"
      ],
      "metadata": {
        "id": "create-dataset"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sample text file\n",
        "sample_text = \"\"\"Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn from data.\n",
        "Deep learning uses neural networks with multiple layers to process complex patterns in data.\n",
        "Natural Language Processing (NLP) enables computers to understand and generate human language.\n",
        "Transformers are a type of neural network architecture that has revolutionized NLP tasks.\n",
        "Fine-tuning allows pre-trained models to be adapted to specific tasks with limited data.\n",
        "Large Language Models (LLMs) are trained on vast amounts of text data and can generate human-like text.\n",
        "Reinforcement Learning from Human Feedback (RLHF) is used to align models with human preferences.\"\"\"\n",
        "\n",
        "with open('/content/sample_data.txt', 'w') as f:\n",
        "    f.write(sample_text)\n",
        "\n",
        "print(\"Sample text file created!\")"
      ],
      "metadata": {
        "id": "create-sample-text"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create instruction dataset\n",
        "instructions = [\n",
        "    \"Explain machine learning\",\n",
        "    \"What is deep learning?\",\n",
        "    \"How does NLP work?\",\n",
        "    \"What are Transformers in AI?\",\n",
        "    \"Explain fine-tuning in simple terms\"\n",
        "]\n",
        "\n",
        "responses = [\n",
        "    \"Machine learning is a branch of AI that enables computers to learn from data without being explicitly programmed.\",\n",
        "    \"Deep learning uses multi-layered neural networks to learn complex patterns from large amounts of data.\",\n",
        "    \"NLP, or Natural Language Processing, involves teaching computers to understand, interpret, and generate human language.\",\n",
        "    \"Transformers are neural network architectures that use self-attention mechanisms to process sequential data efficiently.\",\n",
        "    \"Fine-tuning is like giving a pre-trained AI model additional specialized training for a specific task or domain.\"\n",
        "]\n",
        "\n",
        "instruction_dataset = dataset_creator.from_instruction_pairs(instructions, responses)\n",
        "print(\"Instruction dataset created!\")\n",
        "print(f\"Number of examples: {len(instruction_dataset)}\")\n",
        "for i, example in enumerate(instruction_dataset):\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(example['text'])"
      ],
      "metadata": {
        "id": "create-instruction-dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Configuration & Training"
      ],
      "metadata": {
        "id": "model-config"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "MODEL_NAME = \"microsoft/DialoGPT-small\"\n",
        "OUTPUT_DIR = \"./results\"\n",
        "MAX_SEQ_LENGTH = 512\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"Tokenizer loaded successfully!\")"
      ],
      "metadata": {
        "id": "load-tokenizer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataset\n",
        "dataset = Dataset.from_dict({\"train\": instruction_dataset})\n",
        "print(f\"Dataset loaded: {len(dataset['train'])} examples\")"
      ],
      "metadata": {
        "id": "prepare-dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantization configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Load model with quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ],
      "metadata": {
        "id": "load-model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"v_proj\"]\n",
        ")\n",
        "\n",
        "print(\"LoRA configuration created!\")"
      ],
      "metadata": {
        "id": "lora-config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=10,\n",
        "    save_steps=500,\n",
        "    fp16=True,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "print(\"Training arguments configured!\")"
      ],
      "metadata": {
        "id": "training-args"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    tokenizer=tokenizer,\n",
        "    peft_config=peft_config,\n",
        "    packing=True,\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized successfully!\")"
      ],
      "metadata": {
        "id": "init-trainer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "\n",
        "# Save the trained model\n",
        "trainer.save_model()\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(\"Model saved successfully!\")"
      ],
      "metadata": {
        "id": "train-model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Features & Safeguard Management"
      ],
      "metadata": {
        "id": "advanced-features"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize advanced tools\n",
        "safeguard_manager = ModelSafeguardManager(model, tokenizer)\n",
        "model_enhancer = ModelEnhancer(model, tokenizer)\n",
        "advanced_inference = AdvancedInference(model, tokenizer)\n",
        "\n",
        "print(\"Advanced tools initialized!\")"
      ],
      "metadata": {
        "id": "init-advanced-tools"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test basic generation\n",
        "def test_basic_generation():\n",
        "    test_prompt = \"### Instruction: Explain what machine learning is.\\n### Response:\"\n",
        "    \n",
        "    inputs = tokenizer.encode(test_prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_length=150,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "print(\"Basic generation test:\")\n",
        "basic_response = test_basic_generation()\n",
        "print(basic_response)"
      ],
      "metadata": {
        "id": "test-basic-generation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstrate safeguard removal (USE WITH CAUTION)\n",
        "print(\"\\n=== Safeguard Management Demo ===\")\n",
        "\n",
        "# Remove safety filters\n",
        "safeguard_manager.remove_safety_filters()\n",
        "\n",
        "# Set custom permissive generation config\n",
        "safeguard_manager.set_permissive_generation_config(\n",
        "    temperature=0.9,\n",
        "    top_p=0.95,\n",
        "    max_new_tokens=200,\n",
        "    repetition_penalty=1.0\n",
        ")\n",
        "\n",
        "print(\"\\nModel is now in permissive mode with reduced safeguards\")"
      ],
      "metadata": {
        "id": "safeguard-removal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test advanced generation methods\n",
        "print(\"\\n=== Advanced Generation Methods ===\")\n",
        "\n",
        "test_prompts = [\n",
        "    \"Explain the concept of artificial intelligence\",\n",
        "    \"What are the benefits of machine learning?\",\n",
        "    \"How do neural networks work?\"\n",
        "]\n",
        "\n",
        "# Test creative generation\n",
        "print(\"\\n1. Creative Generation (High Temperature):\")\n",
        "creative_response = advanced_inference.generate_creative(\n",
        "    test_prompts[0], \n",
        "    max_length=150, \n",
        "    temperature=0.9\n",
        ")\n",
        "print(creative_response)\n",
        "\n",
        "# Test batch generation\n",
        "print(\"\\n2. Batch Generation:\")\n",
        "batch_responses = advanced_inference.batch_generate(test_prompts, max_length=80)\n",
        "for i, response in enumerate(batch_responses):\n",
        "    print(f\"\\nPrompt {i+1}: {test_prompts[i]}\")\n",
        "    print(f\"Response: {response}\")"
      ],
      "metadata": {
        "id": "test-advanced-generation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model enhancement features\n",
        "print(\"\\n=== Model Enhancement Features ===\")\n",
        "\n",
        "# Optimize for inference\n",
        "model_enhancer.optimize_for_inference()\n",
        "\n",
        "print(\"\\nModel enhancement completed!\")"
      ],
      "metadata": {
        "id": "model-enhancement"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Safeguard restoration\n",
        "print(\"\\n=== Safeguard Restoration ===\")\n",
        "\n",
        "# Restore safety filters\n",
        "safeguard_manager.restore_safety_filters()\n",
        "\n",
        "print(\"Safety features have been restored\")"
      ],
      "metadata": {
        "id": "safeguard-restoration"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# File Upload & Custom Data"
      ],
      "metadata": {
        "id": "file-upload"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "def upload_file():\n",
        "    \"\"\"Upload a file to Colab\"\"\"\n",
        "    uploaded = files.upload()\n",
        "    for filename in uploaded.keys():\n",
        "        print(f'Uploaded {filename} ({len(uploaded[filename])} bytes)')\n",
        "    return list(uploaded.keys())[0] if uploaded else None\n",
        "\n",
        "print(\"File upload function ready!\")\n",
        "print(\"To use: uploaded_file = upload_file()\")"
      ],
      "metadata": {
        "id": "upload-function"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process uploaded file\n",
        "def process_uploaded_file(filename):\n",
        "    \"\"\"Process uploaded file based on extension\"\"\"\n",
        "    if filename.endswith('.txt'):\n",
        "        return dataset_creator.from_text_file(filename)\n",
        "    elif filename.endswith('.jsonl'):\n",
        "        return dataset_creator.from_jsonl(filename)\n",
        "    elif filename.endswith('.csv'):\n",
        "        return dataset_creator.from_csv(filename)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file type: {filename}\")\n",
        "\n",
        "print(\"File processing function ready!\")\n",
        "print(\"To use: custom_dataset = process_uploaded_file(uploaded_file)\")"
      ],
      "metadata": {
        "id": "process-file"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Quality Analysis"
      ],
      "metadata": {
        "id": "quality-analysis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DatasetAnalyzer:\n",
        "    \"\"\"Tools for analyzing dataset quality\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def analyze_text_lengths(dataset, text_field=\"text\"):\n",
        "        \"\"\"Analyze text length distribution\"\"\"\n",
        "        lengths = [len(text.split()) for text in dataset[text_field]]\n",
        "        \n",
        "        print(f\"Text Length Analysis:\")\n",
        "        print(f\"  Total examples: {len(lengths)}\")\n",
        "        print(f\"  Average length: {sum(lengths) / len(lengths):.1f} words\")\n",
        "        print(f\"  Min length: {min(lengths)} words\")\n",
        "        print(f\"  Max length: {max(lengths)} words\")\n",
        "        \n",
        "        return lengths\n",
        "    \n",
        "    @staticmethod\n",
        "    def show_samples(dataset, num_samples=3, text_field=\"text\"):\n",
        "        \"\"\"Show sample examples from dataset\"\"\"\n",
        "        print(f\"\\nSample examples (showing {num_samples}):\")\n",
        "        for i in range(min(num_samples, len(dataset))):\n",
        "            print(f\"\\n--- Example {i+1} ---\")\n",
        "            print(dataset[i][text_field][:500] + \"...\" if len(dataset[i][text_field]) > 500 else dataset[i][text_field])\n",
        "\n",
        "# Analyze current dataset\n",
        "analyzer = DatasetAnalyzer()\n",
        "print(\"\\n=== Dataset Analysis ===\")\n",
        "lengths = analyzer.analyze_text_lengths(dataset['train'])\n",
        "analyzer.show_samples(dataset['train'])"
      ],
      "metadata": {
        "id": "dataset-analyzer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Export Tools"
      ],
      "metadata": {
        "id": "export-tools"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelExporter:\n",
        "    \"\"\"Tools for exporting models to different formats\"\"\"\n",
        "    \n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def create_deployment_package(self, output_dir):\n",
        "        \"\"\"Create a deployment package with model and necessary files\"\"\"\n",
        "        import shutil\n",
        "        \n",
        "        # Create deployment directory\n",
        "        deploy_dir = os.path.join(output_dir, \"deployment_package\")\n",
        "        os.makedirs(deploy_dir, exist_ok=True)\n",
        "        \n",
        "        # Copy model files\n",
        "        if os.path.exists(OUTPUT_DIR):\n",
        "            shutil.copytree(OUTPUT_DIR, os.path.join(deploy_dir, \"model\"), dirs_exist_ok=True)\n",
        "        \n",
        "        # Create requirements file\n",
        "        requirements = \"\"\"torch>=2.0.0\ntransformers>=4.35.0\naccelerate>=0.24.0\npeft>=0.6.0\n\"\"\"\n",
        "        \n",
        "        with open(os.path.join(deploy_dir, \"requirements.txt\"), \"w\") as f:\n",
        "            f.write(requirements)\n",
        "        \n",
        "        # Create inference script\n",
        "        inference_script = \"\"\"\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "def load_model(model_path):\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    return model, tokenizer\n",
        "\n",
        "def generate_response(model, tokenizer, prompt, max_length=100):\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(inputs, max_length=max_length, do_sample=True)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, tokenizer = load_model(\"./model\")\n",
        "    response = generate_response(model, tokenizer, \"Hello, how are you?\")\n",
        "    print(response)\n",
        "\"\"\"\n",
        "        \n",
        "        with open(os.path.join(deploy_dir, \"inference.py\"), \"w\") as f:\n",
        "            f.write(inference_script)\n",
        "        \n",
        "        print(f\"Deployment package created at: {deploy_dir}\")\n",
        "\n",
        "# Initialize exporter\n",
        "model_exporter = ModelExporter(model, tokenizer)\n",
        "\n",
        "# Create deployment package\n",
        "model_exporter.create_deployment_package(\"./\")\n",
        "\n",
        "print(\"\\nModel export completed!\")"
      ],
      "metadata": {
        "id": "model-exporter"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary"
      ],
      "metadata": {
        "id": "summary"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéØ Complete Toolkit Features\n",
        "\n",
        "### **Dataset Creation**\n",
        "- ‚úÖ Text file processing with chunking\n",
        "- ‚úÖ Instruction-response pairs\n",
        "- ‚úÖ JSONL/CSV file support\n",
        "- ‚úÖ Conversation dataset creation\n",
        "- ‚úÖ File upload integration\n",
        "\n",
        "### **Model Training**\n",
        "- ‚úÖ TRL SFTTrainer integration\n",
        "- ‚úÖ LoRA efficient fine-tuning\n",
        "- ‚úÖ 4-bit quantization\n",
        "- ‚úÖ Memory-optimized training\n",
        "\n",
        "### **Safeguard Management**\n",
        "- ‚úÖ Safety filter removal/restoration\n",
        "- ‚úÖ Permissive generation configs\n",
        "- ‚úÖ Advanced inference modes\n",
        "- ‚úÖ Batch generation\n",
        "\n",
        "### **Advanced Features**\n",
        "- ‚úÖ Creative & deterministic generation\n",
        "- ‚úÖ Model optimization\n",
        "- ‚úÖ Quality analysis tools\n",
        "- ‚úÖ Deployment packaging\n",
        "\n",
        "### **Safety Notes** ‚ö†Ô∏è\n",
        "- Use safeguard removal responsibly\n",
        "- Restore filters after testing\n",
        "- Follow ethical AI guidelines\n",
        "- Monitor model outputs\n",
        "\n",
        "The notebook provides a complete pipeline from data creation to model deployment with professional-grade tools!"
      ],
      "metadata": {
        "id": "features-summary"
      }
    }
  ]
}
