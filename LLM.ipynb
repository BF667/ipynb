{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPS2TJw18CMxp6MLPhqmLIc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BF667/ipynb/blob/main/LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jx7WHY6RVEM8"
      },
      "outputs": [],
      "source": [
        "!uv pip install torch transformers datasets tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Ensure 'your_text_file.txt' exists for demonstration.\n",
        "# In a real scenario, this would be your actual training data.\n",
        "text_file_path = \"/content/data.txt\"\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, BertTokenizerFast\n",
        "\n",
        "# Re-load the dataset if it's not already defined from a previous cell.\n",
        "# This ensures the code can run independently if needed.\n",
        "try:\n",
        "    _ = dataset # Check if dataset is already defined\n",
        "except NameError:\n",
        "    print(f\"Loading dataset from {text_file_path}\")\n",
        "    dataset = load_dataset(\"text\", data_files={\"train\": text_file_path})\n",
        "\n",
        "# Create an iterator over the text data for tokenizer training.\n",
        "# This function yields batches of text from your dataset.\n",
        "def get_training_corpus():\n",
        "    # Iterate through the 'train' split of the dataset\n",
        "    for i in range(0, len(dataset[\"train\"]), 1000):\n",
        "        yield dataset[\"train\"][i : i + 1000][\"text\"]\n",
        "\n",
        "# To train a NEW tokenizer from scratch (e.g., based on BertTokenizerFast architecture),\n",
        "# you typically start by instantiating the tokenizer class and then calling `train_new_from_iterator`.\n",
        "# We'll use a `BertTokenizerFast` as an example.\n",
        "\n",
        "print(\"Initializing new tokenizer for training...\")\n",
        "# Start with a base tokenizer and define special tokens.\n",
        "# vocab_size here is a placeholder; the actual vocab will be built from the corpus.\n",
        "new_tokenizer = BertTokenizerFast.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    unk_token=\"[UNK]\",\n",
        "    sep_token=\"[SEP]\",\n",
        "    pad_token=\"[PAD]\",\n",
        "    cls_token=\"[CLS]\",\n",
        "    mask_token=\"[MASK]\"\n",
        ")\n",
        "\n",
        "# Train the tokenizer on your corpus.\n",
        "# `vocab_size` here is a target size, `min_frequency` helps filter rare words.\n",
        "print(\"Training new tokenizer from corpus...\")\n",
        "new_tokenizer = new_tokenizer.train_new_from_iterator(\n",
        "    get_training_corpus(),\n",
        "    vocab_size=30522, # This can be adjusted based on your desired vocabulary size\n",
        "    min_frequency=2 # Words appearing less than this will be ignored\n",
        "    # Removed special_tokens argument as it's already defined in new_tokenizer\n",
        ")\n",
        "print(\"Tokenizer training complete.\")\n",
        "\n",
        "# Save the trained tokenizer to a local directory.\n",
        "tokenizer_name = \"my_custom_bert_tokenizer\"\n",
        "save_path = \"./\" + tokenizer_name\n",
        "new_tokenizer.save_pretrained(save_path)\n",
        "print(f\"New tokenizer saved to: {save_path}\")\n",
        "\n",
        "# You can now load and use your newly trained tokenizer:\n",
        "# loaded_tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
        "# print(f\"\\nTesting new tokenizer: {loaded_tokenizer('Hello world, this is my new custom tokenizer!')}\")\n",
        "# print(f\"New tokenizer vocabulary size: {len(loaded_tokenizer)}\")"
      ],
      "metadata": {
        "id": "qH6mXGfhVURQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RHwdTacJXVQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "# Load a pre-trained tokenizer (or train a new one)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/my_custom_bert_tokenizer\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "17bSzWp5VQNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# Example: Define a simple model (or load a pre-trained base for fine-tuning)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"
      ],
      "metadata": {
        "id": "XQjl9htBXiYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "OsCoolDYXmDA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}