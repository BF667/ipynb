{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPS2TJw18CMxp6MLPhqmLIc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BF667/ipynb/blob/main/LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Fine-tuning with TRL - Complete Toolkit\n",
        "\n",
        "This notebook demonstrates fine-tuning language models using TRL with comprehensive dataset creation and safeguard management tools."
      ],
      "metadata": {
        "id": "Xq8W5Q6dWz4c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jx7WHY6RVEM8"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch transformers datasets tokenizers trl peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "import pandas as pd\n",
        "import gc\n",
        "from datasets import Dataset, load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    BitsAndBytesConfig,\n",
        "    GenerationConfig\n",
        ")\n",
        "from trl import SFTTrainer\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    PeftModel,\n",
        "    PeftConfig,\n",
        "    TaskType\n",
        ")\n",
        "\n",
        "# Disable Weights & Biases\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "print(\"All packages imported successfully!\")"
      ],
      "metadata": {
        "id": "d8vGJ-kUW2ND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Creation Tools"
      ],
      "metadata": {
        "id": "dataset-tools-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DatasetCreator:\n",
        "    \"\"\"Tool for creating training datasets from various sources\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def from_text_file(file_path, chunk_size=512, overlap=50):\n",
        "        \"\"\"\n",
        "        Create dataset from a plain text file\n",
        "        \n",
        "        Args:\n",
        "            file_path: Path to text file\n",
        "            chunk_size: Size of text chunks\n",
        "            overlap: Overlap between chunks\n",
        "        \"\"\"\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "        \n",
        "        # Split text into chunks\n",
        "        chunks = []\n",
        "        start = 0\n",
        "        text_length = len(text)\n",
        "        \n",
        "        while start < text_length:\n",
        "            end = start + chunk_size\n",
        "            chunk = text[start:end]\n",
        "            chunks.append({\"text\": chunk})\n",
        "            start += chunk_size - overlap\n",
        "        \n",
        "        return Dataset.from_list(chunks)\n",
        "    \n",
        "    @staticmethod\n",
        "    def from_instruction_pairs(instructions, responses, system_prompt=None):\n",
        "        \"\"\"Create dataset from instruction-response pairs\"\"\"\n",
        "        data = []\n",
        "        for instr, resp in zip(instructions, responses):\n",
        "            if system_prompt:\n",
        "                formatted_text = f\"### System: {system_prompt}\\n### Instruction: {instr}\\n### Response: {resp}\"\n",
        "            else:\n",
        "                formatted_text = f\"### Instruction: {instr}\\n### Response: {resp}\"\n",
        "            data.append({\"text\": formatted_text})\n",
        "        \n",
        "        return Dataset.from_list(data)\n",
        "    \n",
        "    @staticmethod\n",
        "    def from_jsonl(file_path, text_field=\"text\"):\n",
        "        \"\"\"Create dataset from JSONL file\"\"\"\n",
        "        data = []\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                item = json.loads(line)\n",
        "                data.append({\"text\": item[text_field]})\n",
        "        \n",
        "        return Dataset.from_list(data)\n",
        "    \n",
        "    @staticmethod\n",
        "    def from_csv(file_path, text_column=\"text\"):\n",
        "        \"\"\"Create dataset from CSV file\"\"\"\n",
        "        df = pd.read_csv(file_path)\n",
        "        data = [{\"text\": str(row[text_column])} for _, row in df.iterrows()]\n",
        "        return Dataset.from_list(data)\n",
        "    \n",
        "    @staticmethod\n",
        "    def create_conversation_dataset(conversations, system_message=None):\n",
        "        \"\"\"Create dataset from conversation history\"\"\"\n",
        "        data = []\n",
        "        for conv in conversations:\n",
        "            formatted_conv = \"\"\n",
        "            if system_message:\n",
        "                formatted_conv += f\"System: {system_message}\\n\"\n",
        "            formatted_conv += \"\\n\".join([f\"{role}: {text}\" for role, text in conv])\n",
        "            data.append({\"text\": formatted_conv})\n",
        "        \n",
        "        return Dataset.from_list(data)\n",
        "\n",
        "# Example usage of dataset creator\n",
        "dataset_creator = DatasetCreator()"
      ],
      "metadata": {
        "id": "dataset-creator-class"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Safeguard Removal & Model Management Tools"
      ],
      "metadata": {
        "id": "safeguard-tools-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelSafeguardManager:\n",
        "    \"\"\"Tools for managing model safeguards and safety features\"\"\"\n",
        "    \n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.original_generation_config = None\n",
        "        \n",
        "    def remove_safety_filters(self):\n",
        "        \"\"\"Remove safety filters from the model (use with caution)\"\"\"\n",
        "        print(\"Removing safety filters...\")\n",
        "        \n",
        "        # Store original generation config\n",
        "        self.original_generation_config = self.model.generation_config\n",
        "        \n",
        "        # Create a more permissive generation config\n",
        "        permissive_config = GenerationConfig.from_dict({\n",
        "            \"do_sample\": True,\n",
        "            \"temperature\": 0.8,\n",
        "            \"top_p\": 0.95,\n",
        "            \"top_k\": 50,\n",
        "            \"max_new_tokens\": 512,\n",
        "            \"repetition_penalty\": 1.1,\n",
        "            \"pad_token_id\": self.tokenizer.eos_token_id,\n",
        "            \"eos_token_id\": self.tokenizer.eos_token_id,\n",
        "        })\n",
        "        \n",
        "        self.model.generation_config = permissive_config\n",
        "        print(\"Safety filters removed - model is now in permissive mode\")\n",
        "        \n",
        "    def restore_safety_filters(self):\n",
        "        \"\"\"Restore original safety filters\"\"\"\n",
        "        if self.original_generation_config:\n",
        "            self.model.generation_config = self.original_generation_config\n",
        "            print(\"Safety filters restored\")\n",
        "        else:\n",
        "            print(\"No original config found to restore\")\n",
        "    \n",
        "    def set_permissive_generation_config(self, **kwargs):\n",
        "        \"\"\"Set custom permissive generation parameters\"\"\"\n",
        "        default_config = {\n",
        "            \"do_sample\": True,\n",
        "            \"temperature\": 0.9,\n",
        "            \"top_p\": 0.95,\n",
        "            \"top_k\": 0,\n",
        "            \"max_new_tokens\": 1024,\n",
        "            \"repetition_penalty\": 1.0,\n",
        "            \"no_repeat_ngram_size\": 0,\n",
        "        }\n",
        "        default_config.update(kwargs)\n",
        "        \n",
        "        permissive_config = GenerationConfig.from_dict(default_config)\n",
        "        self.model.generation_config = permissive_config\n",
        "        print(\"Custom permissive generation config applied\")\n",
        "    \n",
        "    def remove_instruction_templates(self, prompt):\n",
        "        \"\"\"Remove any instruction template formatting from prompts\"\"\"\n",
        "        # Common instruction templates to remove\n",
        "        templates = [\n",
        "            \"### Instruction:\",\n",
        "            \"### Response:\",\n",
        "            \"### System:\",\n",
        "            \"<|system|>\",\n",
        "            \"<|user|>\",\n",
        "            \"<|assistant|>\",\n",
        "            \"System:\",\n",
        "            \"User:\",\n",
        "            \"Assistant:\",\n",
        "        ]\n",
        "        \n",
        "        cleaned_prompt = prompt\n",
        "        for template in templates:\n",
        "            cleaned_prompt = cleaned_prompt.replace(template, \"\")\n",
        "        \n",
        "        return cleaned_prompt.strip()\n",
        "\n",
        "class ModelEnhancer:\n",
        "    \"\"\"Tools for enhancing model capabilities and features\"\"\"\n",
        "    \n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def merge_lora_weights(self, output_dir):\n",
        "        \"\"\"Merge LoRA weights into base model for faster inference\"\"\"\n",
        "        print(\"Merging LoRA weights into base model...\")\n",
        "        \n",
        "        if hasattr(self.model, 'merge_and_unload'):\n",
        "            self.model = self.model.merge_and_unload()\n",
        "            \n",
        "            # Save merged model\n",
        "            self.model.save_pretrained(output_dir)\n",
        "            self.tokenizer.save_pretrained(output_dir)\n",
        "            print(f\"Merged model saved to {output_dir}\")\n",
        "        else:\n",
        "            print(\"Model doesn't appear to have LoRA weights to merge\")\n",
        "    \n",
        "    def optimize_for_inference(self):\n",
        "        \"\"\"Optimize model for faster inference\"\"\"\n",
        "        print(\"Optimizing model for inference...\")\n",
        "        \n",
        "        # Set model to evaluation mode\n",
        "        self.model.eval()\n",
        "        \n",
        "        # Enable better performance options\n",
        "        if hasattr(self.model, 'config'):\n",
        "            self.model.config.use_cache = True\n",
        "        \n",
        "        print(\"Model optimized for inference\")\n",
        "    \n",
        "    def create_model_card(self, output_path, model_name, description=\"\"):\n",
        "        \"\"\"Create a model card for the trained model\"\"\"\n",
        "        model_card = f\"\"\"---\n",
        "license: mit\n",
        "library_name: transformers\n",
        "tags:\n",
        "- fine-tuned\n",
        "- trl\n",
        "- peft\n",
        "---\n",
        "\n",
        "# {model_name}\n",
        "\n",
        {description}\n",
        \n",
        "## Training Details\n",
        "\n",
        "This model was fine-tuned using TRL (Transformer Reinforcement Learning) and PEFT (Parameter-Efficient Fine-Tuning).\n",
        "\n",
        "### Training Hyperparameters\n",
        "- **Training regime:** LoRA (Low-Rank Adaptation)\n",
        "- **r value:** 16\n",
        "- **alpha:** 32\n",
        "- **dropout:** 0.05\n",
        "\n",
        "### Framework Versions\n",
        "- PEFT {getattr(__import__('peft'), '__version__', 'unknown')}\n",
        "- TRL {getattr(__import__('trl'), '__version__', 'unknown')}\n",
        "- Transformers {getattr(__import__('transformers'), '__version__', 'unknown')}\n",
        "- Torch {getattr(__import__('torch'), '__version__', 'unknown')}\n",
        "\"\"\"\n",
        "        \n",
        "        with open(output_path, 'w') as f:\n",
        "            f.write(model_card)\n",
        "        print(f\"Model card saved to {output_path}\")\n",
        "\n",
        "class AdvancedInference:\n",
        "    \"\"\"Advanced inference tools with multiple generation strategies\"\"\"\n",
        "    \n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def generate_creative(self, prompt, max_length=200, temperature=0.9):\n",
        "        \"\"\"Generate creative text with high temperature\"\"\"\n",
        "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                inputs,\n",
        "                max_length=max_length,\n",
        "                temperature=temperature,\n",
        "                do_sample=True,\n",
        "                top_p=0.95,\n",
        "                top_k=50,\n",
        "                repetition_penalty=1.1,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
        "    def generate_deterministic(self, prompt, max_length=200):\n",
        "        \"\"\"Generate deterministic text with greedy decoding\"\"\"\n",
        "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                inputs,\n",
        "                max_length=max_length,\n",
        "                do_sample=False,\n",
        "                num_beams=1,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    def generate_with_beam_search(self, prompt, max_length=200, num_beams=4):\n",
        "        \"\"\"Generate text using beam search for better quality\"\"\"\n",
        "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                inputs,\n",
        "                max_length=max_length,\n",
        "                num_beams=num_beams,\n",
        "                early_stopping=True,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    def batch_generate(self, prompts, max_length=100):\n",
        "        \"\"\"Generate responses for multiple prompts\"\"\"\n",
        "        inputs = self.tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(self.model.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_length=max_length,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        responses = []\n",
        "        for i, output in enumerate(outputs):\n",
        "            response = self.tokenizer.decode(output, skip_special_tokens=True)\n",
        "            # Remove the original prompt\n",
        "            response = response[len(prompts[i]):].strip()\n",
        "            responses.append(response)\n",
        "        \n",
        "        return responses"
      ],
      "metadata": {
        "id": "safeguard-manager-class"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Creation Examples"
      ],
      "metadata": {
        "id": "dataset-examples-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Create sample text file for demonstration\n",
        "sample_text = \"\"\"Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn from data.\n",
        "Deep learning uses neural networks with multiple layers to process complex patterns in data.\n",
        "Natural Language Processing (NLP) enables computers to understand and generate human language.\n",
        "Transformers are a type of neural network architecture that has revolutionized NLP tasks.\n",
        "Fine-tuning allows pre-trained models to be adapted to specific tasks with limited data.\n",
        "Large Language Models (LLMs) are trained on vast amounts of text data and can generate human-like text.\n",
        "Reinforcement Learning from Human Feedback (RLHF) is used to align models with human preferences.\n",
        "Quantization reduces model size and memory requirements by using lower precision numerical formats.\n",
        "LoRA (Low-Rank Adaptation) is an efficient fine-tuning method that reduces trainable parameters.\n",
        "Safeguards are implemented in AI systems to prevent harmful or unethical outputs.\"\"\"\n",
        "\n",
        "with open('/content/sample_data.txt', 'w') as f:\n",
        "    f.write(sample_text)\n",
        "\n",
        "print(\"Sample text file created!\")"
      ],
      "metadata": {
        "id": "create-sample-text"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Create dataset from text file\n",
        "text_dataset = dataset_creator.from_text_file('/content/sample_data.txt', chunk_size=200)\n",
        "print(\"Text file dataset:\")\n",
        "print(f\"Number of examples: {len(text_dataset)}\")\n",
        "print(f\"First example: {text_dataset[0]['text']}\")"
      ],
      "metadata": {
        "id": "create-from-text-file"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: Create instruction-response dataset\n",
        "instructions = [\n",
        "    \"Explain machine learning\",\n",
        "    \"What is deep learning?\",\n",
        "    \"How does NLP work?\",\n",
        "    \"What are Transformers in AI?\",\n",
        "    \"Explain fine-tuning in simple terms\"\n",
        "]\n",
        "\n",
        "responses = [\n",
        "    \"Machine learning is a branch of AI that enables computers to learn from data without being explicitly programmed.\",\n",
        "    \"Deep learning uses multi-layered neural networks to learn complex patterns from large amounts of data.\",\n",
        "    \"NLP, or Natural Language Processing, involves teaching computers to understand, interpret, and generate human language.\",\n",
        "    \"Transformers are neural network architectures that use self-attention mechanisms to process sequential data efficiently.\",\n",
        "    \"Fine-tuning is like giving a pre-trained AI model additional specialized training for a specific task or domain.\"\n",
        "]\n",
        "\n",
        "instruction_dataset = dataset_creator.from_instruction_pairs(instructions, responses)\n",
        "print(\"\\nInstruction dataset:\")\n",
        "for i, example in enumerate(instruction_dataset):\n",
        "    print(f\"Example {i+1}: {example['text']}\")"
      ],
      "metadata": {
        "id": "create-instruction-dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Configuration & Training"
      ],
      "metadata": {
        "id": "model-config-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "MODEL_NAME = \"microsoft/DialoGPT-small\"  # You can change this to any causal LM\n",
        "OUTPUT_DIR = \"./results\"\n",
        "MAX_SEQ_LENGTH = 512\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set pad token\n",
        "\n",
        "print(\"Tokenizer loaded successfully!\")"
      ],
      "metadata": {
        "id": "k9V2c4a_W5dO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose your dataset source (UNCOMMENT ONE OPTION):\n",
        "\n",
        "# OPTION 1: Use HuggingFace dataset\n",
        "# dataset = load_dataset(\"timdettmers/openassistant-guanaco\")\n",
        "# def format_instruction(example):\n",
        "#     return {\"text\": f\"### Instruction: {example['instruction']}\\n### Response: {example['response']}\"}\n",
        "# dataset = dataset.map(format_instruction)\n",
        "\n",
        "# OPTION 2: Use your own text file\n",
        "# dataset = Dataset.from_dict({\"train\": dataset_creator.from_text_file(\"/content/your_data.txt\")})\n",
        "\n",
        "# OPTION 3: Use instruction dataset we created above\n",
        "dataset = Dataset.from_dict({\"train\": instruction_dataset})\n",
        "\n",
        "# OPTION 4: Use conversation dataset\n",
        "# dataset = Dataset.from_dict({\"train\": conversation_dataset})\n",
        "\n",
        "print(f\"Dataset loaded: {len(dataset['train'])} examples\")"
      ],
      "metadata": {
        "id": "q9b8pQy_W8vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantization configuration for memory efficiency (optional)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Load model with quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ],
      "metadata": {
        "id": "vP4UQ8HXXA7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA configuration for efficient fine-tuning\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"v_proj\"]  # Adjust based on your model architecture\n",
        ")\n",
        "\n",
        "print(\"LoRA configuration created!\")"
      ],
      "metadata": {
        "id": "4Dm8s4L6XDVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=10,\n",
        "    save_steps=500,\n",
        "    fp16=True,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    report_to=\"none\",  # Disable all logging integrations including wandb\n",
        "    remove_unused_columns=False,\n",
        ")"
      ],
      "metadata": {
        "id": "8uV5U0GzXGDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    tokenizer=tokenizer,\n",
        "    peft_config=peft_config,\n",
        "    packing=True,  # Efficiently pack multiple sequences\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized successfully!\")"
      ],
      "metadata": {
        "id": "7mQK4z_OXI8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "training-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "vjvG3bXIXL7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "trainer.save_model()\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(\"Model saved successfully!\")"
      ],
      "metadata": {
        "id": "o6V6yV3xXOT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Safeguard Management & Advanced Features"
      ],
      "metadata": {
        "id": "safeguard-features-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize safeguard manager and enhancers\n",
        "safeguard_manager = ModelSafeguardManager(model, tokenizer)\n",
        "model_enhancer = ModelEnhancer(model, tokenizer)\n",
        "advanced_inference = AdvancedInference(model, tokenizer)\n",
        "\n",
        "print(\"Advanced tools initialized!\")"
      ],
      "metadata": {
        "id": "init-advanced-tools"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test basic generation\n",
        "def test_basic_generation():\n",
        "    \"\"\"Test model with basic generation\"\"\"\n",
        "    test_prompt = \"### Instruction: Explain what machine learning is.\\n### Response:\"\n",
        "    \n",
        "    inputs = tokenizer.encode(test_prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_length=150,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "print(\"Basic generation test:\")\n",
        "basic_response = test_basic_generation()\n",
        "print(basic_response)"
      ],
      "metadata": {
        "id": "test-basic-generation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstrate safeguard removal (USE WITH CAUTION)\n",
        "print(\"\\n=== Safeguard Management Demo ===\")\n",
        "\n",
        "# Remove safety filters\n",
        "safeguard_manager.remove_safety_filters()\n",
        "\n",
        "# Set custom permissive generation config\n",
        "safeguard_manager.set_permissive_generation_config(\n",
        "    temperature=0.9,\n",
        "    top_p=0.95,\n",
        "    max_new_tokens=200,\n",
        "    repetition_penalty=1.0\n",
        ")\n",
        "\n",
        "print(\"\\nModel is now in permissive mode with reduced safeguards\")"
      ],
      "metadata": {
        "id": "demo-safeguard-removal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test advanced generation methods\n",
        "print(\"\\n=== Advanced Generation Methods ===\")\n",
        "\n",
        "test_prompts = [\n",
        "    \"Explain the concept of artificial intelligence\",\n",
        "    \"What are the benefits of machine learning?\",\n",
        "    \"How do neural networks work?\"\n",
        "]\n",
        "\n",
        "# Test creative generation\n",
        "print(\"\\n1. Creative Generation (High Temperature):\")\n",
        "creative_response = advanced_inference.generate_creative(\n",
        "    test_prompts[0], \n",
        "    max_length=150, \n",
        "    temperature=0.9\n",
        ")\n",
        "print(creative_response)\n",
        "\n",
        "# Test deterministic generation\n",
        "print(\"\\n2. Deterministic Generation:\")\n",
        "deterministic_response = advanced_inference.generate_deterministic(\n",
        "    test_prompts[1], \n",
        "    max_length=100\n",
        ")\n",
        "print(deterministic_response)\n",
        "\n",
        "# Test batch generation\n",
        "print(\"\\n3. Batch Generation:\")\n",
        "batch_responses = advanced_inference.batch_generate(test_prompts, max_length=80)\n",
        "for i, response in enumerate(batch_responses):\n",
        "    print(f\"\\nPrompt {i+1}: {test_prompts[i]}\")\n",
        "    print(f\"Response: {response}\")"
      ],
      "metadata": {
        "id": "test-advanced-generation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model enhancement features\n",
        "print(\"\\n=== Model Enhancement Features ===\")\n",
        "\n",
        "# Optimize for inference\n",
        "model_enhancer.optimize_for_inference()\n",
        "\n",
        "# Create model card\n",
        "model_enhancer.create_model_card(\n",
        "    \"./model_card.md\",\n",
        "    \"My Fine-tuned DialoGPT\",\n",
        "    \"This model was fine-tuned on AI/ML concepts using TRL and LoRA.\"\n",
        ")\n",
        "\n",
        "print(\"\\nModel enhancement completed!\")"
      ],
      "metadata": {
        "id": "model-enhancement-features"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Safeguard restoration\n",
        "print(\"\\n=== Safeguard Restoration ===\")\n",
        "\n",
        "# Restore safety filters\n",
        "safeguard_manager.restore_safety_filters()\n",
        "\n",
        "print(\"Safety features have been restored\")"
      ],
      "metadata": {
        "id": "safeguard-restoration"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload Your Own Data"
      ],
      "metadata": {
        "id": "upload-data-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload your own files (run this cell to upload files to Colab)\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "def upload_file():\n",
        "    \"\"\"Upload a file to Colab\"\"\"\n",
        "    uploaded = files.upload()\n",
        "    for filename in uploaded.keys():\n",
        "        print(f'Uploaded {filename} ({len(uploaded[filename])} bytes)')\n",
        "    return list(uploaded.keys())[0] if uploaded else None\n",
        "\n",
        "# Uncomment the line below to upload a file\n",
        "# uploaded_file = upload_file()"
      ],
      "metadata": {
        "id": "upload-file-cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process uploaded file based on type\n",
        "def process_uploaded_file(filename):\n",
        "    \"\"\"Process uploaded file based on extension\"\"\"\n",
        "    if filename.endswith('.txt'):\n",
        "        return dataset_creator.from_text_file(filename)\n",
        "    elif filename.endswith('.jsonl'):\n",
        "        return dataset_creator.from_jsonl(filename)\n",
        "    elif filename.endswith('.csv'):\n",
        "        return dataset_creator.from_csv(filename)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file type: {filename}\")\n",
        "\n",
        "# Example usage after upload:\n",
        "# if uploaded_file:\n",
        "#     custom_dataset = process_uploaded_file(uploaded_file)\n",
        "#     print(f\"Processed {len(custom_dataset)} examples from {uploaded_file}\")"
      ],
      "metadata": {
        "id": "process-uploaded-file"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Quality Tools"
      ],
      "metadata": {
        "id": "quality-tools-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DatasetAnalyzer:\n",
        "    \"\"\"Tools for analyzing dataset quality\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def analyze_text_lengths(dataset, text_field=\"text\"):\n",
        "        \"\"\"Analyze text length distribution\"\"\"\n",
        "        lengths = [len(text.split()) for text in dataset[text_field]]\n",
        "        \n",
        "        print(f\"Text Length Analysis:\")\n",
        "        print(f\"  Total examples: {len(lengths)}\")\n",
        "        print(f\"  Average length: {sum(lengths) / len(lengths):.1f} words\")\n",
        "        print(f\"  Min length: {min(lengths)} words\")\n",
        "        print(f\"  Max length: {max(lengths)} words\")\n",
        "        \n",
        "        return lengths\n",
        "    \n",
        "    @staticmethod\n",
        "    def show_samples(dataset, num_samples=3, text_field=\"text\"):\n",
        "        \"\"\"Show sample examples from dataset\"\"\"\n",
        "        print(f\"\\nSample examples (showing {num_samples}):\")\n",
        "        for i in range(min(num_samples, len(dataset))):\n",
        "            print(f\"\\n--- Example {i+1} ---\")\n",
        "            print(dataset[i][text_field][:500] + \"...\" if len(dataset[i][text_field]) > 500 else dataset[i][text_field])\n",
        "\n",
        "# Analyze current dataset\n",
        "analyzer = DatasetAnalyzer()\n",
        "lengths = analyzer.analyze_text_lengths(dataset['train'])\n",
        "analyzer.show_samples(dataset['train'])"
      ],
      "metadata": {
        "id": "dataset-analyzer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Export & Deployment Tools"
      ],
      "metadata": {
        "id": "export-tools-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelExporter:\n",
        "    \"\"\"Tools for exporting models to different formats\"\"\"\n",
        "    \n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def export_to_gguf(self, output_path, quantization=\"q4_0\"):\n",
        "        \"\"\"Export model to GGUF format (for llama.cpp)\"\"\"\n",
        "        try:\n",
        "            # This would typically use llama.cpp conversion tools\n",
        "            print(f\"GGUF export to {output_path} with {quantization} quantization\")\n",
        "            print(\"Note: Full GGUF export requires additional setup with llama.cpp\")\n",
        "        except Exception as e:\n",
        "            print(f\"GGUF export failed: {e}\")\n",
        "    \n",
        "    def export_to_onnx(self, output_path):\n",
        "        \"\"\"Export model to ONNX format\"\"\"\n",
        "        try:\n",
        "            print(f\"ONNX export to {output_path}\")\n",
        "            print(\"Note: ONNX export requires torch.onnx.export\")\n",
        "        except Exception as e:\n",
        "            print(f\"ONNX export failed: {e}\")\n",
        "    \n",
        "    def create_deployment_package(self, output_dir):\n",
        "        \"\"\"Create a deployment package with model and necessary files\"\"\"\n",
        "        import shutil\n",
        "        \n",
        "        # Create deployment directory\n",
        "        deploy_dir = os.path.join(output_dir, \"deployment_package\")\n",
        "        os.makedirs(deploy_dir, exist_ok=True)\n",
        "        \n",
        "        # Copy model files\n",
        "        if os.path.exists(OUTPUT_DIR):\n",
        "            shutil.copytree(OUTPUT_DIR, os.path.join(deploy_dir, \"model\"), dirs_exist_ok=True)\n",
        "        \n",
        "        # Create requirements file\n",
        "        requirements = \"\"\"torch>=2.0.0\ntransformers>=4.35.0\naccelerate>=0.24.0\npeft>=0.6.0\n\"\"\"\n",
        "        \n",
        "        with open(os.path.join(deploy_dir, \"requirements.txt\"), \"w\") as f:\n",
        "            f.write(requirements)\n",
        "        \n",
        "        # Create inference script\n",
        "        inference_script = \"\"\"\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "def load_model(model_path):\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    return model, tokenizer\n",
        "\n",
        "def generate_response(model, tokenizer, prompt, max_length=100):\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(inputs, max_length=max_length, do_sample=True)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, tokenizer = load_model(\"./model\")\n",
        "    response = generate_response(model, tokenizer, \"Hello, how are you?\")\n",
        "    print(response)\n",
        "\"\"\"\n",
        "        \n",
        "        with open(os.path.join(deploy_dir, \"inference.py\"), \"w\") as f:\n",
        "            f.write(inference_script)\n",
        "        \n",
        "        print(f\"Deployment package created at: {deploy_dir}\")\n",
        "\n",
        "# Initialize exporter\n",
        "model_exporter = ModelExporter(model, tokenizer)\n",
        "\n",
        "# Create deployment package\n",
        "model_exporter.create_deployment_package(\"./\")\n",
        "\n",
        "print(\"\\nModel export tools ready!\")"
      ],
      "metadata": {
        "id": "model-exporter"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Complete Toolkit Summary\n",
        "\n",
        "### **Dataset Creation Tools:**\n",
        "- Text file chunking\n",
        "- Instruction-response pairs\n",
        "- Conversation history\n",
        - JSONL/CSV support\n",
        "- Quality analysis\n",
        "\n",
        "### **Safeguard Management:**\n",
        "- Safety filter removal/restoration\n",
        "- Permissive generation configs\n",
        "- Instruction template removal\n",
        "- Custom generation parameters\n",
        "\n",
        "### **Advanced Features:**\n",
        "- Multiple generation strategies\n",
        - Batch inference\n",
        - Model optimization\n",
        - LoRA weight merging\n",
        - Model card creation\n",
        "\n",
        "### **Export & Deployment:**\n",
        "- Deployment packages\n",
        - ONNX export ready\n",
        - GGUF conversion support\n",
        - Complete inference scripts\n",
        "\n",
        "### **Safety Notes:**\n",
        "‚ö†Ô∏è **Use safeguard removal tools responsibly**\n",
        "- Only use for research/development\n",
        "- Be aware of potential risks\n",
        "- Restore safeguards after testing\n",
        "- Follow ethical AI guidelines"
      ],
      "metadata": {
        "id": "complete-summary"
      }
    }
  ]
}
