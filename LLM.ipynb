{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPS2TJw18CMxp6MLPhqmLIc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BF667/ipynb/blob/main/LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_packages"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers datasets tokenizers trl peft bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, \n",
        "    AutoConfig,\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2Config,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "# Create sample training data\n",
        "text_file_path = \"/content/data.txt\"\n",
        "if not os.path.exists(text_file_path):\n",
        "    # Create a larger dataset for better training\n",
        "    sample_texts = [\n",
        "        \"The quick brown fox jumps over the lazy dog. \",\n",
        "        \"Machine learning is a subset of artificial intelligence. \",\n",
        "        \"Python is a popular programming language for data science. \",\n",
        "        \"Deep learning models require large amounts of data. \",\n",
        "        \"Natural language processing helps computers understand text. \",\n",
        "        \"Transformers revolutionized the field of NLP. \",\n",
        "        \"Neural networks are inspired by the human brain. \",\n",
        "        \"Gradient descent is an optimization algorithm. \",\n",
        "        \"Backpropagation is used to train neural networks. \",\n",
        "        \"Attention mechanisms help models focus on important parts. \"\n",
        "    ]\n",
        "    \n",
        "    with open(text_file_path, \"w\") as f:\n",
        "        # Repeat the sample texts many times to create a decent dataset\n",
        "        for _ in range(1000):\n",
        "            for text in sample_texts:\n",
        "                f.write(text)\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"text\", data_files={\"train\": text_file_path})\n",
        "\n",
        "# Train a tokenizer from scratch\n",
        "print(\"Training tokenizer from scratch...\")\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "trainer = BpeTrainer(\n",
        "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
        "    vocab_size=10000,  # Small vocabulary for demonstration\n",
        "    min_frequency=2,\n",
        ")\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# Train the tokenizer on our dataset\n",
        "def get_training_corpus():\n",
        "    for i in range(0, len(dataset[\"train\"]), 1000):\n",
        "        yield dataset[\"train\"][i : i + 1000][\"text\"]\n",
        "\n",
        "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)\n",
        "\n",
        "# Convert to HuggingFace tokenizer\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "hf_tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_object=tokenizer,\n",
        "    unk_token=\"[UNK]\",\n",
        "    pad_token=\"[PAD]\",\n",
        "    cls_token=\"[CLS]\",\n",
        "    sep_token=\"[SEP]\",\n",
        "    mask_token=\"[MASK]\",\n",
        ")\n",
        "\n",
        "# Create a custom configuration for our model\n",
        "config = GPT2Config(\n",
        "    vocab_size=hf_tokenizer.vocab_size,\n",
        "    n_positions=512,  # Maximum sequence length\n",
        "    n_embd=256,       # Embedding dimension\n",
        "    n_layer=6,       # Number of transformer layers\n",
        "    n_head=8,        # Number of attention heads\n",
        "    resid_pdrop=0.1,\n",
        "    embd_pdrop=0.1,\n",
        "    attn_pdrop=0.1,\n",
        ")\n",
        "\n",
        "# Initialize model from scratch with random weights\n",
        "print(\"Initializing model from scratch...\")\n",
        "model = GPT2LMHeadModel(config)\n",
        "model.resize_token_embeddings(len(hf_tokenizer))\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    return hf_tokenizer(\n",
        "        examples[\"text\"], \n",
        "        truncation=True, \n",
        "        max_length=512, \n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "print(\"Tokenizing dataset...\")\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Configure LoRA for parameter-efficient fine-tuning\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=5e-4,  # Higher learning rate for training from scratch\n",
        "    logging_steps=10,\n",
        "    num_train_epochs=10,  # More epochs for training from scratch\n",
        "    save_steps=100,\n",
        "    fp16=True,\n",
        "    report_to=None,  # Disable wandb reporting\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# Data collator for language modeling\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=hf_tokenizer,\n",
        "    mlm=False,  # We're doing causal language modeling, not masked\n",
        ")\n",
        "\n",
        "# Initialize the TRL trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    peft_config=lora_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=512,\n",
        "    tokenizer=hf_tokenizer,\n",
        "    args=training_args,\n",
        "    packing=False,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Train the model from scratch\n",
        "print(\"Training model from scratch...\")\n",
        "trainer.train()\n",
        "\n",
        "# Save the model and tokenizer\n",
        "trainer.save_model(\"./scratch_model\")\n",
        "hf_tokenizer.save_pretrained(\"./scratch_model\")\n",
        "print(\"Model and tokenizer saved to ./scratch_model\")"
      ],
      "metadata": {
        "id": "train_from_scratch"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model trained from scratch\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# Load the model trained from scratch\n",
        "model_path = \"./scratch_model\"\n",
        "generator = pipeline(\n",
        "    'text-generation', \n",
        "    model=model_path, \n",
        "    tokenizer=model_path,\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "\n",
        "# Generate text\n",
        "prompts = [\n",
        "    \"Machine learning\",\n",
        "    \"The quick brown\",\n",
        "    \"Deep learning\",\n",
        "    \"Neural networks\"\n",
        "]\n",
        "\n",
        "for prompt in prompts:\n",
        "    outputs = generator(\n",
        "        prompt, \n",
        "        max_length=50, \n",
        "        num_return_sequences=1,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        pad_token_id=generator.tokenizer.eos_token_id\n",
        "    )\n",
        "    print(f\"\\nPrompt: {prompt}\")\n",
        "    print(f\"Generated: {outputs[0]['generated_text']}\")"
      ],
      "metadata": {
        "id": "test_scratch_model"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Evaluate the model's perplexity\n",
        "import math\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Create a small evaluation set\n",
        "eval_texts = [\n",
        "    \"The future of artificial intelligence is bright.\",\n",
        "    \"Machine learning algorithms improve with more data.\",\n",
        "    \"Neural networks can learn complex patterns.\",\n",
        "    \"Transformers use attention mechanisms effectively.\",\n",
        "    \"Python is widely used in data science.\"\n",
        "]\n",
        "\n",
        "# Tokenize evaluation texts\n",
        "eval_encodings = hf_tokenizer(eval_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "# Calculate perplexity\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(**eval_encodings, labels=eval_encodings[\"input_ids\"])\n",
        "    loss = outputs.loss\n",
        "    perplexity = math.exp(loss)\n",
        "    \n",
        "print(f\"Model perplexity: {perplexity:.2f}\")"
      ],
      "metadata": {
        "id": "evaluate_model"
      },
      "outputs": []
    }
  ]
}
